{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Tweets Topic Modeling\n",
    "\n",
    "### Background\n",
    "This is based on Sentiment140 dataset with 1.6 million tweets(https://www.kaggle.com/kazanova/sentiment140). This project is a continuation of the Twitter Sentiment Analysis that my group and I have done as a final project for Applied Machine Learning course at Wright State University. (direct to Github page for Twitter Sentiment Analysis report)\n",
    "\n",
    "### Plan\n",
    "Using preprocessed tweets dataset, I want to know the most topics that people tweeted with the help of topic modeling using LDA (Latent Dirichlet Allocation).\n",
    "* Generate 5 to 20 topics from the dataset\n",
    "* Create a word cloud based on frequency of texts\n",
    "* Create LDA Visualization\n",
    "* Generate a graph and table of perplexity vs. number of topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1046516\n",
      "                                              Tweets  Index\n",
      "0  awww thats a bummer  you shoulda got david car...      0\n",
      "1  is upset that he cant update his facebook by t...      1\n",
      "2   i dived many times for the ball managed to sa...      2\n",
      "3    my whole body feels itchy and like its on fire       3\n",
      "4   no its not behaving at all im mad why am i he...      4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('tweets-clean.csv', error_bad_lines=False);\n",
    "data_text = data[['Tweets']]\n",
    "data_text['Index'] = data_text.index\n",
    "documents = data_text\n",
    "\n",
    "print(len(documents))\n",
    "print(documents[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\arcit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import PorterStemmer\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return PorterStemmer().stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['i', 'am', 'so', 'sick', 'from', 'stress', 'gah', 'and', 'i', 'need', 'sleep', 'and', 'it', 'is', 'all', 'my', 'fault', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['sick', 'stress', 'need', 'sleep', 'fault']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = documents[documents['Index'] == 4310].values[0][0]\n",
    "\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           [awww, that, bummer, shoulda, david, carr]\n",
       "1    [upset, updat, facebook, text, result, school,...\n",
       "2         [dive, time, ball, manag, save, rest, bound]\n",
       "3                            [bodi, feel, itchi, like]\n",
       "4                                              [behav]\n",
       "5                                               [crew]\n",
       "6                                               [need]\n",
       "7                 [long, time, rain, fine, thank, how]\n",
       "8                                        [nope, didnt]\n",
       "9                                              [muera]\n",
       "Name: Tweets, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs = documents['Tweets'].map(preprocess)\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.026*\"like\" + 0.023*\"littl\" + 0.018*\"girl\" + 0.018*\"think\" + 0.016*\"sweet\" + 0.016*\"didnt\" + 0.015*\"funni\" + 0.014*\"cute\" + 0.014*\"guess\" + 0.014*\"quot\"\n",
      "Topic: 1 \n",
      "Words: 0.154*\"http\" + 0.066*\"quot\" + 0.037*\"twitpiccom\" + 0.030*\"bitli\" + 0.029*\"say\" + 0.016*\"tinyurlcom\" + 0.015*\"check\" + 0.013*\"head\" + 0.013*\"look\" + 0.012*\"like\"\n",
      "Topic: 2 \n",
      "Words: 0.115*\"love\" + 0.052*\"know\" + 0.034*\"haha\" + 0.029*\"your\" + 0.029*\"that\" + 0.024*\"miss\" + 0.022*\"cool\" + 0.020*\"thank\" + 0.020*\"song\" + 0.018*\"dont\"\n",
      "Topic: 3 \n",
      "Words: 0.055*\"work\" + 0.044*\"go\" + 0.034*\"today\" + 0.031*\"tomorrow\" + 0.031*\"weekend\" + 0.029*\"week\" + 0.029*\"home\" + 0.024*\"time\" + 0.024*\"wait\" + 0.020*\"friday\"\n",
      "Topic: 4 \n",
      "Words: 0.076*\"watch\" + 0.061*\"want\" + 0.040*\"dont\" + 0.030*\"wish\" + 0.024*\"movi\" + 0.017*\"tonight\" + 0.017*\"know\" + 0.015*\"like\" + 0.011*\"think\" + 0.011*\"drink\"\n",
      "Topic: 5 \n",
      "Words: 0.054*\"need\" + 0.024*\"help\" + 0.023*\"talk\" + 0.017*\"read\" + 0.016*\"final\" + 0.015*\"book\" + 0.015*\"write\" + 0.014*\"let\" + 0.014*\"updat\" + 0.013*\"finish\"\n",
      "Topic: 6 \n",
      "Words: 0.151*\"good\" + 0.060*\"thank\" + 0.054*\"hope\" + 0.046*\"morn\" + 0.038*\"feel\" + 0.037*\"night\" + 0.032*\"like\" + 0.028*\"better\" + 0.017*\"great\" + 0.015*\"yeah\"\n",
      "Topic: 7 \n",
      "Words: 0.048*\"get\" + 0.035*\"play\" + 0.031*\"hour\" + 0.029*\"readi\" + 0.022*\"music\" + 0.019*\"game\" + 0.016*\"yesterday\" + 0.013*\"happen\" + 0.012*\"exam\" + 0.012*\"today\"\n",
      "Topic: 8 \n",
      "Words: 0.056*\"twitter\" + 0.051*\"follow\" + 0.043*\"nice\" + 0.034*\"tweet\" + 0.032*\"thank\" + 0.022*\"sleep\" + 0.015*\"post\" + 0.015*\"tri\" + 0.015*\"work\" + 0.014*\"send\"\n",
      "Topic: 9 \n",
      "Words: 0.061*\"happi\" + 0.054*\"come\" + 0.039*\"look\" + 0.036*\"friend\" + 0.027*\"best\" + 0.020*\"hear\" + 0.019*\"mother\" + 0.019*\"welcom\" + 0.019*\"birthday\" + 0.018*\"great\"\n"
     ]
    }
   ],
   "source": [
    "# how does it gets the topics? Is it per tweet or whole doc? Look up how LdaMulticore works\n",
    "# Per topic, it comes from different tweets depending on if the word appear on the tweet\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
