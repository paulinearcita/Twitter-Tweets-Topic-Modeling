{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "This document shows how text processing affects a given sentence or text. This is intend for text processing learning as well as document text preprocessing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Special Characters\n",
    "Special characters are noise to the tweets and has to be removed to lessen the noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing special characters: \n",
      "The cats are hanging their feet while playing $#?@!!\n",
      "\n",
      "After special characters removed: \n",
      "The cats are hanging their feet while playing \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def remove_special_character(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "#this is the tweet that we will remove special characters from\n",
    "doc_sample = 'The cats are hanging their feet while playing $#?@!!'\n",
    "\n",
    "print('Before removing special characters: ')\n",
    "print(doc_sample)\n",
    "\n",
    "print('\\nAfter special characters removed: ')\n",
    "doc_sample = remove_special_character(doc_sample, remove_digits=True)\n",
    "print(doc_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stop Words\n",
    "Stop words are words that needs to be filtered before or after natural language processing. These are words that are most common in the sentence like \"a\", \"an\", \"the\", etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: nltk in c:\\users\\arcit\\anaconda3\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied, skipping upgrade: regex in c:\\users\\arcit\\anaconda3\\lib\\site-packages (from nltk) (2020.5.14)\n",
      "Requirement already satisfied, skipping upgrade: joblib in c:\\users\\arcit\\anaconda3\\lib\\site-packages (from nltk) (0.15.1)\n",
      "Requirement already satisfied, skipping upgrade: click in c:\\users\\arcit\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in c:\\users\\arcit\\anaconda3\\lib\\site-packages (from nltk) (4.46.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "#download stopwords\n",
    "import nltk \n",
    "\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "print(stopword_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "Tokenization is chopping up sentences into pieces or words called tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Tokenization: \n",
      "The cats are hanging their feet while playing \n",
      "\n",
      "After Tokenization: \n",
      "['The', 'cats', 'are', 'hanging', 'their', 'feet', 'while', 'playing']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize - only applied to single tweet for showing \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(doc_sample)\n",
    "\n",
    "print('Before Tokenization: ')\n",
    "print(doc_sample)\n",
    "\n",
    "print('\\nAfter Tokenization: ')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and Removing Stop Words\n",
    "Using tokenized sentence, we're now going to remove stop words. It is important to tokenize the sentence/document first before removing stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used this package \n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing stopwords: \n",
      "['The', 'cats', 'are', 'hanging', 'their', 'feet', 'while', 'playing']\n",
      "\n",
      "After removing stopwords: \n",
      "The, cats, hanging, feet, playing\n"
     ]
    }
   ],
   "source": [
    "#Tokenize and remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    filtered_text = ', '.join(filtered_tokens)   \n",
    "    return filtered_text\n",
    "\n",
    "print('Before removing stopwords: ')\n",
    "print(tokens)\n",
    "\n",
    "print('\\nAfter removing stopwords: ')\n",
    "processed_sample = remove_stopwords(doc_sample)\n",
    "print(processed_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "Stemming is the process of reducing inflection in words to their root forms such as mapping a group of words to the same stem even if the stem itself is not a valid word in the language(english language).\n",
    "\n",
    "Inflection - modification of a word to express different grammatical categories such as tense, case, voice, aspect, person, number, gender, and mood. An inflection expresses one or more grammatical categories with a prefix, suffix or infix, or another internal modification such as a vowel change\n",
    "\n",
    "##### Stems are created by removing suffixes or prefixes used with a word.\n",
    "##### Stemming word or sentence may result in words that are not actual words or words that doesn't have meaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before stemming: \n",
      "The, cats, hanging, feet, playing\n",
      "\n",
      "After stemming: \n",
      "the, cats, hanging, feet, play\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def text_stemmer(text):\n",
    "    portStem = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([portStem.stem(word) for word in text.split()])\n",
    "    return text\n",
    "    \n",
    "print('Before stemming: ')\n",
    "print(processed_sample)\n",
    "\n",
    "print('\\nAfter stemming: ')\n",
    "stemmed_doc = text_stemmer(processed_sample)\n",
    "print(stemmed_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "Unlike Stemming, it reduces the inflected words properly ensuring that the root word belongs to the language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before lemmatization: \n",
      "The, cats, hanging, feet, playing\n",
      "\n",
      "After lemmatization: \n",
      "The , cat , hanging , foot , playing\n"
     ]
    }
   ],
   "source": [
    "def text_lemmatizer(text):\n",
    "    word_list = nltk.word_tokenize(text)\n",
    "    lemmatized_text = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "    return lemmatized_text\n",
    "\n",
    "print('Before lemmatization: ')\n",
    "print(processed_sample)\n",
    "\n",
    "print('\\nAfter lemmatization: ')\n",
    "lemmatized_doc = text_lemmatizer(processed_sample)\n",
    "print(lemmatized_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Text Preprocessing to the tweets\n",
    "This is where we tokenize and lemmatize all tweets in the dataset.\n",
    "\n",
    "Note:\n",
    "* On single text preprocessing(process per tweet), we stemmed and lemmatized the tweet separately to see the changes. Comparing the words that were stemmed and lemmatized, some of the words changed but some doesn't. \n",
    "* I will combine all text preprocessing that we've done to make it simple. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the , bat , hanging , foot , 2193\n"
     ]
    }
   ],
   "source": [
    "#text_sample = []\n",
    "def prepare_text(text):\n",
    "    special_character = remove_special_character(text)\n",
    "    stop_words = remove_stopwords(special_character)\n",
    "    stem_text = text_stemmer(stop_words)\n",
    "    lemmatize_text = text_lemmatizer(stem_text)\n",
    "    #text_sample.append(lemmatize_text)\n",
    "    return lemmatize_text\n",
    "\n",
    "sample = 'The bat is hanging its feet for #2193@'\n",
    "\n",
    "print(prepare_text(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet\n",
      "awww , thats , bummer , shoulda , got , david , carr , third , day\n",
      "upset , cant , update , facebook , texting , might , cry , result , school , today , also , blah\n",
      "dived , many , time , ball , managed , save , 50 , rest , go , bound\n",
      "whole , body , feel , itchy , like , fire\n",
      "behaving , im , mad , cant , see\n",
      "whole , crew\n",
      "need , hug\n",
      "hey , long , time , see , yes , rain , bit , bit , lol , im , fine , thanks , how\n",
      "nope , didnt\n"
     ]
    }
   ],
   "source": [
    "text_data = []\n",
    "i = 0\n",
    "with open('tweets-clean.csv', errors='ignore') as f:\n",
    "    for line in f:\n",
    "        tokens = prepare_text(line)\n",
    "        if i < 10:\n",
    "            print(tokens)\n",
    "            i += 1\n",
    "        text_data.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
