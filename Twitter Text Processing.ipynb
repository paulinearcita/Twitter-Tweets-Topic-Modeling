{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "This document shows how text processing affects a given sentence or text. This is intend for text processing learning as well as document text preprocessing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Special Characters\n",
    "Special characters are noise to the text and has to be removed to remove or lessen the noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing special characters: \n",
      "The cats are hanging their feet while playing $#?@!!\n",
      "\n",
      "After special characters removed: \n",
      "The cats are hanging their feet while playing \n"
     ]
    }
   ],
   "source": [
    "#re is regular expression operations library\n",
    "import re\n",
    "\n",
    "def remove_special_character(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "#this is the sample text that we will remove special characters from\n",
    "doc_sample = 'The cats are hanging their feet while playing $#?@!!'\n",
    "\n",
    "print('Before removing special characters: ')\n",
    "print(doc_sample)\n",
    "\n",
    "print('\\nAfter special characters removed: ')\n",
    "doc_sample = remove_special_character(doc_sample, remove_digits=True)\n",
    "print(doc_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stop Words\n",
    "Stop words are words that needs to be filtered before or after natural language processing. These are words that are most common in the sentence like \"a\", \"an\", \"the\", etc. The stop words that is going to be removed in the text are printed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: nltk in c:\\users\\arcit\\anaconda3\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied, skipping upgrade: regex in c:\\users\\arcit\\anaconda3\\lib\\site-packages (from nltk) (2020.5.14)\n",
      "Requirement already satisfied, skipping upgrade: joblib in c:\\users\\arcit\\anaconda3\\lib\\site-packages (from nltk) (0.15.1)\n",
      "Requirement already satisfied, skipping upgrade: click in c:\\users\\arcit\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in c:\\users\\arcit\\anaconda3\\lib\\site-packages (from nltk) (4.46.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "#importing nltk library\n",
    "import nltk \n",
    "\n",
    "#getting the list of stopwords from nltk library\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "#prints the list of stop words\n",
    "print(stopword_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "Tokenization is chopping up sentences into pieces or words called tokens. This process is necessary to be able to remove stop words.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Tokenization: \n",
      "The cats are hanging their feet while playing \n",
      "\n",
      "After Tokenization: \n",
      "['The', 'cats', 'are', 'hanging', 'their', 'feet', 'while', 'playing']\n"
     ]
    }
   ],
   "source": [
    "#import word_tokenize which has the code to tokenize from nltk.tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#store the tokenized text into tokens variable\n",
    "tokens = word_tokenize(doc_sample)\n",
    "\n",
    "# Showing the before and after\n",
    "print('Before Tokenization: ')\n",
    "print(doc_sample)\n",
    "\n",
    "print('\\nAfter Tokenization: ')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and Removing Stop Words\n",
    "Using tokenized sentence, we're now going to remove stop words. It is important to tokenize the sentence/document first before removing stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used ToktokTokenizer from nltk to tokenize text\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "\n",
    "#set tokenizer as ToktokTokenizer\n",
    "tokenizer = ToktokTokenizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing stopwords: \n",
      "['The', 'cats', 'are', 'hanging', 'their', 'feet', 'while', 'playing']\n",
      "\n",
      "After removing stopwords: \n",
      "The, cats, hanging, feet, playing\n"
     ]
    }
   ],
   "source": [
    "#Tokenize and remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    filtered_text = ', '.join(filtered_tokens)   \n",
    "    return filtered_text\n",
    "\n",
    "print('Before removing stopwords: ')\n",
    "print(tokens)\n",
    "\n",
    "print('\\nAfter removing stopwords: ')\n",
    "processed_sample = remove_stopwords(doc_sample)\n",
    "print(processed_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "Stemming is the process of reducing inflection in words to their root forms such as mapping a group of words to the same stem even if the stem itself is not a valid word in the language(english language).\n",
    "\n",
    "Inflection - modification of a word to express different grammatical categories such as tense, case, voice, aspect, person, number, gender, and mood. An inflection expresses one or more grammatical categories with a prefix, suffix or infix, or another internal modification such as a vowel change\n",
    "\n",
    "##### Stems are created by removing suffixes or prefixes used with a word.\n",
    "##### Stemming word or sentence may result in words that are not actual words or words that doesn't have meaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before stemming: \n",
      "The, cats, hanging, feet, playing\n",
      "\n",
      "After stemming: \n",
      "the, cats, hanging, feet, play\n"
     ]
    }
   ],
   "source": [
    "#used PorterStemmer from nltk to stem the text\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def text_stemmer(text):\n",
    "    portStem = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([portStem.stem(word) for word in text.split()])\n",
    "    return text\n",
    "    \n",
    "print('Before stemming: ')\n",
    "print(processed_sample)\n",
    "\n",
    "print('\\nAfter stemming: ')\n",
    "stemmed_doc = text_stemmer(processed_sample)\n",
    "print(stemmed_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "Unlike Stemming, it reduces the inflected words properly ensuring that the root word belongs to the language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use WordNetLemmatizer for lemmatization of text\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#set lemmatizer to WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before lemmatization: \n",
      "The, cats, hanging, feet, playing\n",
      "\n",
      "After lemmatization: \n",
      "The , cat , hanging , foot , playing\n"
     ]
    }
   ],
   "source": [
    "#function to lemmatize text\n",
    "def text_lemmatizer(text):\n",
    "    word_list = nltk.word_tokenize(text)\n",
    "    lemmatized_text = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "    return lemmatized_text\n",
    "\n",
    "#showing the before and after\n",
    "print('Before lemmatization: ')\n",
    "print(processed_sample)\n",
    "\n",
    "print('\\nAfter lemmatization: ')\n",
    "lemmatized_doc = text_lemmatizer(processed_sample)\n",
    "print(lemmatized_doc)\n",
    "\n",
    "#CHANGES\n",
    "#cats became cat\n",
    "#feet became foot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Text Preprocessing to Twitter dataset\n",
    "This is where we combine text processing to make changes to Twitter dataset. I created a function to be able to combine processes and apply the changes in one function calling.\n",
    "\n",
    "Note:\n",
    "* On single text preprocessing(sample text), we stemmed and lemmatized the tweet separately to see the changes. Comparing the words that were stemmed and lemmatized, some of the words changed but some doesn't.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before text processing: \n",
      "The bat is hanging its feet for #2193@\n",
      "\n",
      "After text processing: \n",
      "the , bat , hanging , foot , 2193\n"
     ]
    }
   ],
   "source": [
    "#function that combined all text processes\n",
    "def prepare_text(text):\n",
    "    special_character = remove_special_character(text)\n",
    "    stop_words = remove_stopwords(special_character)\n",
    "    stem_text = text_stemmer(stop_words)\n",
    "    lemmatize_text = text_lemmatizer(stem_text)\n",
    "    return lemmatize_text\n",
    "\n",
    "#sample text\n",
    "sample = 'The bat is hanging its feet for #2193@'\n",
    "\n",
    "#to show changes\n",
    "print('Before text processing: ')\n",
    "print(sample)\n",
    "\n",
    "print('\\nAfter text processing: ')\n",
    "print(prepare_text(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet\n",
      "awww , thats , bummer , shoulda , got , david , carr , third , day\n",
      "upset , cant , update , facebook , texting , might , cry , result , school , today , also , blah\n",
      "dived , many , time , ball , managed , save , 50 , rest , go , bound\n",
      "whole , body , feel , itchy , like , fire\n",
      "behaving , im , mad , cant , see\n",
      "whole , crew\n",
      "need , hug\n",
      "hey , long , time , see , yes , rain , bit , bit , lol , im , fine , thanks , how\n",
      "nope , didnt\n"
     ]
    }
   ],
   "source": [
    "#importing csv file that has the tweets and process them\n",
    "\n",
    "text_data = []\n",
    "i = 0\n",
    "with open('tweets-clean.csv', errors='ignore') as f:\n",
    "    for line in f:\n",
    "        tokens = prepare_text(line)\n",
    "        if i < 10:\n",
    "            print(tokens)\n",
    "            i += 1\n",
    "        text_data.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
