{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background\n",
    "This is based on Sentiment140 dataset with 1.6 million tweets(https://www.kaggle.com/kazanova/sentiment140). This project is a continuation of the Twitter Sentiment Analysis that my group and I have done as a final project for Applied Machine Learning course at Wright State University. (direct to Github page for Twitter Sentiment Analysis report)\n",
    "\n",
    "### Plan\n",
    "Using preprocessed tweets dataset, I want to know the most topics that people tweeted with the help of topic modeling using LDA (Latent Dirichlet Allocation).\n",
    "* Generate 5 to 20 topics from the dataset\n",
    "* Create a word cloud based on frequency of texts\n",
    "* Create LDA Visualization\n",
    "* Generate a graph and table of perplexity vs. number of topics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# package used for importing csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    Tweets    index\n",
      "0        awww thats a bummer  you shoulda got david car...        0\n",
      "1        is upset that he cant update his facebook by t...        1\n",
      "2         i dived many times for the ball managed to sa...        2\n",
      "3          my whole body feels itchy and like its on fire         3\n",
      "4         no its not behaving at all im mad why am i he...        4\n",
      "...                                                    ...      ...\n",
      "1046511  back home thought id done for the week but jus...  1046511\n",
      "1046512           my grandma is making dinenr with my mum   1046512\n",
      "1046513  mid morning snack time a bowl of cheese noodle...  1046513\n",
      "1046514   same here  say it like from the terminiator m...  1046514\n",
      "1046515                             im great thaanks  wbuu  1046515\n",
      "\n",
      "[1046516 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('tweets-clean.csv', error_bad_lines=False)\n",
    "data_text = data[['Tweets']]\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text\n",
    "print (documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\arcit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Clean this up - do import on the cell that uses it\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "import nltk\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Special Characters\n",
    "Special characters are noise to the tweets and has to be removed to lessen the noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing special characters: \n",
      "goodnight nobody since i have no followers nobody can see me say this \n",
      "\n",
      "After special characters removed: \n",
      "goodnight nobody since i have no followers nobody can see me say this \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def remove_special_character(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "#this is the tweet that we will remove special characters from\n",
    "doc_sample = documents[documents['index'] == 2314].values[0][0]\n",
    "\n",
    "print('Before removing special characters: ')\n",
    "print(doc_sample)\n",
    "\n",
    "print('\\nAfter special characters removed: ')\n",
    "doc_sample = remove_special_character(doc_sample, remove_digits=True)\n",
    "print(doc_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stop Words\n",
    "Stop words are words that needs to be filtered before or after natural language processing. These are words that are most common in the sentence like \"a\", \"an\", \"the\", etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\arcit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'herself', 'been', 'under', 'if', 'he', 'it', 'once', 'why', 'has', 'an', 'his', \"mightn't\", 'hers', 'have', 't', 'needn', 'should', 'during', 'on', 'very', 'had', 's', 'will', 'be', 'after', 'some', 'ours', 'not', 'at', 'again', 'such', 'their', 'because', 'out', 'mightn', 'am', 'to', \"it's\", 'who', 'me', 'our', 'a', 'theirs', 'down', 'both', 'weren', \"wasn't\", 'yourself', 'you', 'but', 'into', 'against', 'isn', 'before', 'doesn', 'were', 'hadn', 'off', 'shan', 'further', \"hasn't\", 'did', 'by', 'above', 'from', 'few', 'while', 'in', 'own', 'd', 'her', 'doing', 'm', \"needn't\", 'most', \"hadn't\", 'of', 'more', 'can', 'aren', 'those', 'when', 'do', 'ourselves', \"won't\", 'my', 'now', 'its', 'which', \"isn't\", 'we', \"you'd\", 'through', 'll', 'o', 'won', 'them', 'myself', \"you're\", 'here', 'there', 'up', 'each', 'where', \"mustn't\", \"couldn't\", 'is', 'nor', 'yourselves', 'and', \"weren't\", 'this', 'wasn', 'does', 'how', 'that', 'so', 'then', 'over', 'mustn', 'only', 'for', 'as', 'hasn', \"wouldn't\", 'no', 'couldn', 'itself', \"that'll\", 'these', 'are', 'she', 'until', 'any', 'y', 'don', 'himself', 'didn', 'ma', 'being', 'all', 'themselves', \"aren't\", 'they', 'or', 'between', 'the', \"shouldn't\", 've', 'i', 'than', \"you've\", 'yours', \"don't\", 'your', 'haven', 'ain', 'about', 'was', 'same', 're', 'what', 'whom', \"didn't\", 'him', \"doesn't\", 'having', 'just', 'other', 'wouldn', \"she's\", 'too', 'shouldn', 'below', \"shan't\", \"you'll\", \"haven't\", 'with', \"should've\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#download stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Print the stop words - these are the words that's gonna be removed\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "Tokenization is chopping up sentences into pieces or words called tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Tokenization: \n",
      "goodnight nobody since i have no followers nobody can see me say this \n",
      "\n",
      "After Tokenization: \n",
      "['goodnight', 'nobody', 'since', 'i', 'have', 'no', 'followers', 'nobody', 'can', 'see', 'me', 'say', 'this']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize - only applied to single tweet for showing \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(doc_sample)\n",
    "\n",
    "print('Before Tokenization: ')\n",
    "print(doc_sample)\n",
    "\n",
    "print('\\nAfter Tokenization: ')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and Removing Stop Words\n",
    "Using tokenized sentence, we're now going to remove stop words. It is important to tokenize the sentence/document first before removing stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used this package \n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing stopwords: \n",
      "['goodnight', 'nobody', 'since', 'i', 'have', 'no', 'followers', 'nobody', 'can', 'see', 'me', 'say', 'this']\n",
      "\n",
      "After removing stopwords: \n",
      "goodnight, nobody, since, followers, nobody, see, say\n"
     ]
    }
   ],
   "source": [
    "#Tokenize and remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    filtered_text = ', '.join(filtered_tokens)   \n",
    "    return filtered_text\n",
    "\n",
    "print('Before removing stopwords: ')\n",
    "print(tokens)\n",
    "\n",
    "print('\\nAfter removing stopwords: ')\n",
    "processed_sample = remove_stopwords(doc_sample)\n",
    "print(processed_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "Stemming is the process of reducing inflection in words to their root forms such as mapping a group of words to the same stem even if the stem itself is not a valid word in the language(english language).\n",
    "\n",
    "Inflection - modification of a word to express different grammatical categories such as tense, case, voice, aspect, person, number, gender, and mood. An inflection expresses one or more grammatical categories with a prefix, suffix or infix, or another internal modification such as a vowel change\n",
    "\n",
    "##### Stems are created by removing suffixes or prefixes used with a word.\n",
    "##### Stemming word or sentence may result in words that are not actual words or words that doesn't have meaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before stemming: \n",
      "goodnight, nobody, since, followers, nobody, see, say\n",
      "\n",
      "After stemming: \n",
      "goodnight, nobody, since, followers, nobody, see, say\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def text_stemmer(text):\n",
    "    portStem = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([portStem.stem(word) for word in text.split()])\n",
    "    return text\n",
    "    \n",
    "print('Before stemming: ')\n",
    "print(processed_sample)\n",
    "\n",
    "print('\\nAfter stemming: ')\n",
    "stemmed_doc = text_stemmer(processed_sample)\n",
    "print(stemmed_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "Unlike Stemming, it reduces the inflected words properly ensuring that the root word belongs to the language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: nltk in c:\\users\\arcit\\anaconda3\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied, skipping upgrade: regex in c:\\users\\arcit\\anaconda3\\lib\\site-packages (from nltk) (2020.5.14)\n",
      "Requirement already satisfied, skipping upgrade: click in c:\\users\\arcit\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: joblib in c:\\users\\arcit\\anaconda3\\lib\\site-packages (from nltk) (0.15.1)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in c:\\users\\arcit\\anaconda3\\lib\\site-packages (from nltk) (4.46.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before lemmatization: \n",
      "goodnight, nobody, since, followers, nobody, see, say\n",
      "\n",
      "After lemmatization: \n",
      "goodnight , nobody , since , follower , nobody , see , say\n"
     ]
    }
   ],
   "source": [
    "def text_lemmatizer(text):\n",
    "    word_list = nltk.word_tokenize(text)\n",
    "    lemmatized_text = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "    return lemmatized_text\n",
    "\n",
    "print('Before lemmatization: ')\n",
    "print(processed_sample)\n",
    "\n",
    "print('\\nAfter lemmatization: ')\n",
    "lemmatized_doc = text_lemmatizer(processed_sample)\n",
    "print(lemmatized_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Text Preprocessing to the tweets\n",
    "This is where we tokenize and lemmatize all tweets in the dataset.\n",
    "\n",
    "Note:\n",
    "* On single text preprocessing(process per tweet), we stemmed and lemmatized the tweet separately to see the changes. Comparing the words that were stemmed and lemmatized, some of the words changed but some doesn't. \n",
    "* I will combine all text preprocessing that we've done to make it simple. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the , bat , hanging , foot , 2193\n"
     ]
    }
   ],
   "source": [
    "#text_sample = []\n",
    "def prepare_text(text):\n",
    "    special_character = remove_special_character(text)\n",
    "    stop_words = remove_stopwords(special_character)\n",
    "    stem_text = text_stemmer(stop_words)\n",
    "    lemmatize_text = text_lemmatizer(stem_text)\n",
    "    #text_sample.append(lemmatize_text)\n",
    "    return lemmatize_text\n",
    "\n",
    "sample = 'The bat is hanging its feet for #2193@'\n",
    "\n",
    "print(prepare_text(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "awww , thats , bummer , shoulda , got , david , carr , third , day\n",
      "upset , cant , update , facebook , texting , might , cry , result , school , today , also , blah\n",
      "dived , many , time , ball , managed , save , 50 , rest , go , bound\n",
      "whole , body , feel , itchy , like , fire\n",
      "behaving , im , mad , cant , see\n",
      "whole , crew\n",
      "need , hug\n",
      "hey , long , time , see , yes , rain , bit , bit , lol , im , fine , thanks , how\n",
      "nope , didnt\n",
      "que , muera\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "text_data = []\n",
    "i = 0\n",
    "with open('tweets-clean.csv', errors='ignore') as f:\n",
    "    for line in f:\n",
    "        tokens = prepare_text(line)\n",
    "        if i < 10:\n",
    "            print(tokens)\n",
    "            i += 1\n",
    "        text_data.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
