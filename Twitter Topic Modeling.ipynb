{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background\n",
    "This is based on Sentiment140 dataset with 1.6 million tweets(https://www.kaggle.com/kazanova/sentiment140). This project is a continuation of the Twitter Sentiment Analysis that my group and I have done as a final project for Applied Machine Learning course at Wright State University. (direct to Github page for Twitter Sentiment Analysis report)\n",
    "\n",
    "### Plan\n",
    "Using preprocessed tweets dataset, I want to know the most topics that people tweeted with the help of topic modeling using LDA (Latent Dirichlet Allocation).\n",
    "* Generate 5 to 20 topics from the dataset\n",
    "* Create a word cloud based on frequency of texts\n",
    "* Create LDA Visualization\n",
    "* Generate a graph and table of perplexity vs. number of topics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# package used for importing csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    Tweets    index\n",
      "0        awww thats a bummer  you shoulda got david car...        0\n",
      "1        is upset that he cant update his facebook by t...        1\n",
      "2         i dived many times for the ball managed to sa...        2\n",
      "3          my whole body feels itchy and like its on fire         3\n",
      "4         no its not behaving at all im mad why am i he...        4\n",
      "...                                                    ...      ...\n",
      "1046511  back home thought id done for the week but jus...  1046511\n",
      "1046512           my grandma is making dinenr with my mum   1046512\n",
      "1046513  mid morning snack time a bowl of cheese noodle...  1046513\n",
      "1046514   same here  say it like from the terminiator m...  1046514\n",
      "1046515                             im great thaanks  wbuu  1046515\n",
      "\n",
      "[1046516 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('tweets-clean.csv', error_bad_lines=False)\n",
    "data_text = data[['Tweets']]\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text\n",
    "print (documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\arcit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Clean this up - do import on the cell that uses it\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "import nltk\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Special Characters\n",
    "Special characters are noise to the tweets and has to be removed to lessen the noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing special characters: \n",
      "blehhhhhhhhh way to early  *feels like puking* \n",
      "\n",
      "After special characters removed: \n",
      "blehhhhhhhhh way to early  feels like puking \n"
     ]
    }
   ],
   "source": [
    "def removeSpecialCharacter(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "#this is the tweet that we will remove special characters from\n",
    "doc_sample = documents[documents['index'] == 4312].values[0][0]\n",
    "\n",
    "print('Before removing special characters: ')\n",
    "print(doc_sample)\n",
    "\n",
    "print('\\nAfter special characters removed: ')\n",
    "doc_sample = removeSpecialCharacter(doc_sample, remove_digits=True)\n",
    "print(doc_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stop Words\n",
    "Stop words are words that needs to be filtered before or after natural language processing. These are words that are most common in the sentence like \"a\", \"an\", \"the\", etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'his', \"needn't\", 'only', 'on', 'to', \"you'd\", 'are', \"haven't\", 'do', 'can', \"don't\", 'which', 'a', 'all', 'than', 'you', 'why', 'from', 'themselves', 'been', 'what', 'so', 'himself', 'this', 'other', 'that', 'not', 'those', 'i', 'them', 'y', 'does', 'again', 'her', 'myself', \"won't\", 'just', 'nor', 'they', 'between', \"couldn't\", \"it's\", 'ourselves', 'through', 'should', 'haven', 'but', 'these', 'more', 'with', 'hasn', 'shouldn', \"should've\", 'itself', 'further', 'wouldn', 'isn', 'have', \"shouldn't\", 'were', 'below', 'some', 'yourself', 'after', 'aren', \"didn't\", 'over', 'by', 've', 'm', 'above', 'it', 'then', 'very', 'she', 'no', 'hadn', 'how', 'here', \"weren't\", 'mightn', 'wasn', 'under', 'ain', 'own', 'needn', 'has', 'while', 'our', 'at', \"you'll\", 'mustn', 'because', 'when', \"hasn't\", 'and', 'yours', 'there', \"hadn't\", 'had', 'where', 'such', 'their', 'we', 'against', 'each', 'few', 'weren', 'he', 'into', \"mustn't\", 'during', 'ours', 'up', 'didn', 'before', 'until', 'most', \"aren't\", 'hers', 'did', \"mightn't\", 'for', 'don', 'doesn', \"you've\", \"that'll\", 'be', 'once', 'too', \"wouldn't\", 'is', \"she's\", 're', \"you're\", \"isn't\", 'any', 'him', 's', 'its', 'll', 'shan', 'your', 'was', 'if', 'out', 'in', 'couldn', 'doing', 'o', 'who', 'd', \"doesn't\", 'my', 'me', 'theirs', 'off', 'whom', 'both', 'about', 'ma', \"wasn't\", 'herself', 'yourselves', 'will', 'having', 'of', \"shan't\", 'same', 'being', 'now', 'down', 'won', 'or', 'am', 'as', 'the', 'an', 't'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\arcit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#download stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Print the stop words - these are the words that's gonna be removed\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "Tokenization is chopping up sentences into pieces or words called tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_sample = documents[documents['index'] == 4310].values[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Tokenization: \n",
      "blehhhhhhhhh way to early  feels like puking \n",
      "\n",
      "After Tokenization: \n",
      "['blehhhhhhhhh', 'way', 'to', 'early', 'feels', 'like', 'puking']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(doc_sample)\n",
    "\n",
    "print('Before Tokenization: ')\n",
    "print(doc_sample)\n",
    "\n",
    "print('\\nAfter Tokenization: ')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and Removing Stop Words\n",
    "Using tokenized sentence, we're now going to remove stop words. It is important to tokenize the sentence/document first before removing stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing stopwords: \n",
      "['blehhhhhhhhh', 'way', 'to', 'early', 'feels', 'like', 'puking']\n",
      "\n",
      "After removing stopwords: \n",
      "['blehhhhhhhhh', 'way', 'early', 'feels', 'like', 'puking']\n"
     ]
    }
   ],
   "source": [
    "processed_sample = [w for w in tokens if not w in stop_words]\n",
    "processed_sample = []\n",
    "\n",
    "for w in tokens:\n",
    "    if w not in stop_words:\n",
    "        processed_sample.append(w)\n",
    "\n",
    "print('Before removing stopwords: ')\n",
    "print(tokens)\n",
    "\n",
    "print('\\nAfter removing stopwords: ')\n",
    "print(processed_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "Stemming is the process of reducing inflection in words to their root forms such as mapping a group of words to the same stem even if the stem itself is not a valid word in the language(english language).\n",
    "\n",
    "Inflection - modification of a word to express different grammatical categories such as tense, case, voice, aspect, person, number, gender, and mood. An inflection expresses one or more grammatical categories with a prefix, suffix or infix, or another internal modification such as a vowel change\n",
    "\n",
    "##### Stems are created by removing suffixes or prefixes used with a word.\n",
    "##### Stemming word or sentence may result in words that are not actual words or words that doesn't have meaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before stemming: \n",
      "['blehhhhhhhhh', 'way', 'early', 'feels', 'like', 'puking']\n",
      "\n",
      "After stemming: \n",
      "['blehhhhhhhhh', 'way', 'earli', 'feel', 'like', 'puke']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def stemDocument(text):\n",
    "    stem_document = []\n",
    "    for word in processed_sample:\n",
    "        stem_document.append(porter.stem(word))\n",
    "    return stem_document\n",
    "    \n",
    "print('Before stemming: ')\n",
    "print(processed_sample)\n",
    "\n",
    "print('\\nAfter stemming: ')\n",
    "stemmed_doc = stemDocument(processed_sample)\n",
    "print(stemmed_doc)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "Unlike Stemming, it reduces the inflected words properly ensuring that the root word belongs to the language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before lemmatization: \n",
      "['blehhhhhhhhh', 'way', 'early', 'feels', 'like', 'puking']\n",
      "\n",
      "After lemmatization: \n",
      "['blehhhhhhhhh', 'way', 'early', 'feel', 'like', 'puking']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmaDocument(text):\n",
    "    lemma_document = []\n",
    "    for word in processed_sample:\n",
    "        lemma_document.append(wordnet_lemmatizer.lemmatize(word))\n",
    "    return lemma_document\n",
    "\n",
    "print('Before lemmatization: ')\n",
    "print(processed_sample)\n",
    "\n",
    "print('\\nAfter lemmatization: ')\n",
    "lemmatized_doc = lemmaDocument(processed_sample)\n",
    "print(lemmatized_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing and Lemmatizing all tweets in dataset\n",
    "This is where we tokenize and lemmatize all tweets in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           [awww, that, bummer, shoulda, david, carr]\n",
       "1    [upset, updat, facebook, text, result, school,...\n",
       "2         [dive, time, ball, manag, save, rest, bound]\n",
       "3                            [bodi, feel, itchi, like]\n",
       "4                                              [behav]\n",
       "5                                               [crew]\n",
       "6                                               [need]\n",
       "7                 [long, time, rain, fine, thank, how]\n",
       "8                                        [nope, didnt]\n",
       "9                                              [muera]\n",
       "Name: Tweets, dtype: object"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OLD STEMMING AND LEMMATIZATION\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        result.append(lemmatize_stemming(token))\n",
    "    return result\n",
    "\n",
    "#doc_sample = documents[documents['index'] == 4312].values[0][0]\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))\n",
    "\n",
    "# Preprocessing each tweet and save them into processed_docs variable\n",
    "processed_docs = documents['Tweets'].map(preprocess)\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
